{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Engineering Team 1 - Documentation Internal Training Schedule Date Topic Status Link 3 Mar 2022 Production Apache Airflow Confirmed Apache Airflow 9 Mar 2022 Docker #1 - Basics Waiting Docker #2 - Advanced Waiting Databases in MEA Waiting Kubernetes #1 - Basics Waiting Kubernetes #2 - Application Deployment Waiting Kubernetes #3 - Administration Waiting","title":"Home"},{"location":"#data-engineering-team-1-documentation","text":"","title":"Data Engineering Team 1 - Documentation"},{"location":"#internal-training-schedule","text":"Date Topic Status Link 3 Mar 2022 Production Apache Airflow Confirmed Apache Airflow 9 Mar 2022 Docker #1 - Basics Waiting Docker #2 - Advanced Waiting Databases in MEA Waiting Kubernetes #1 - Basics Waiting Kubernetes #2 - Application Deployment Waiting Kubernetes #3 - Administration Waiting","title":"Internal Training Schedule"},{"location":"apache-airflow/","text":"Basics Prerequisites \u0e17\u0e31\u0e01\u0e29\u0e30\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e17\u0e35\u0e48\u0e04\u0e27\u0e23\u0e21\u0e35 Python 3.6+ Basic Apache Airflow Basic databases Linux Airflow Architecture \u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d Airflow \u0e2d\u0e48\u0e32\u0e19 DAGs \u0e08\u0e32\u0e01 Bucket \u0e0a\u0e37\u0e48\u0e2d dags \u0e43\u0e19 MinIO \u0e21\u0e35\u0e01\u0e32\u0e23\u0e0b\u0e34\u0e07\u0e04\u0e4c\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e38\u0e01\u0e46 1 \u0e19\u0e32\u0e17\u0e35 Airflow \u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d\u0e01\u0e31\u0e1a Data Platform Services \u0e2b\u0e25\u0e31\u0e07\u0e1a\u0e49\u0e32\u0e19 \u0e40\u0e0a\u0e48\u0e19 Hive, Impala, HBase, Spark Airflow \u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d\u0e01\u0e31\u0e1a\u0e10\u0e32\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19 MEA \u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19 \u0e40\u0e0a\u0e48\u0e19 SAP SFTP, AMR Oracle Database, OT MSSQL Database, etc. Airflow \u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 Docker \u0e41\u0e25\u0e30\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e41\u0e1a\u0e1a Celery Executor \u0e21\u0e35 15 Workers Airflow Server \u0e21\u0e35 92 vCPUs \u0e41\u0e25\u0e30 312 GB RAM \u0e01\u0e23\u0e30\u0e1a\u0e27\u0e19\u0e07\u0e32\u0e19\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19 \u0e40\u0e02\u0e35\u0e22\u0e19 DAGs \u0e14\u0e49\u0e27\u0e22\u0e20\u0e32\u0e29\u0e32 Python Log in \u0e40\u0e02\u0e49\u0e32\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 MinIO Server \u0e41\u0e25\u0e30\u0e43\u0e2b\u0e49\u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e44\u0e1f\u0e25\u0e4c .py \u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e17\u0e35\u0e48 Bucket \u0e0a\u0e37\u0e48\u0e2d dags . \u0e16\u0e49\u0e32\u0e44\u0e21\u0e48\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14 Airflow \u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e0b\u0e34\u0e07\u0e04\u0e4c DAGs \u0e20\u0e32\u0e22\u0e43\u0e19 2 - 3 \u0e19\u0e32\u0e17\u0e35 \u0e16\u0e49\u0e32\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14\u0e2b\u0e19\u0e49\u0e32\u0e15\u0e48\u0e32\u0e07 UI \u0e08\u0e30\u0e41\u0e08\u0e49\u0e07 \u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e41\u0e25\u0e30\u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 DAGs \u0e44\u0e14\u0e49\u0e17\u0e35\u0e48 Airflow Web UI . Example 1 - Simple DAG Create DAGs Copy code \u0e14\u0e49\u0e32\u0e19\u0e25\u0e48\u0e32\u0e07\u0e41\u0e25\u0e30\u0e40\u0e0b\u0e1f\u0e40\u0e1b\u0e47\u0e19\u0e0a\u0e37\u0e48\u0e2d dpd_training_<name>.py dpd_training_chatree.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import airflow from airflow import DAG from airflow.operators.dummy import DummyOperator # Initialize the DAG with DAG ( dag_id = \"dpd_training_chatree\" , description = \"DAG for learning Apache Airflow\" , start_date = airflow . utils . dates . days_ago ( 1 ), schedule_interval = \"@daily\" ) as dag : task1 = DummyOperator ( task_id = \"task1\" ) task2 = DummyOperator ( task_id = \"task2\" ) task3 = DummyOperator ( task_id = \"task3\" ) task4 = DummyOperator ( task_id = \"task4\" ) task1 >> task2 >> task3 >> task4 \u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d Line 8 : DAG ID \u0e15\u0e49\u0e2d\u0e07 \u0e44\u0e21\u0e48\u0e0b\u0e49\u0e33\u0e01\u0e31\u0e19 (Globally unique) Line 10 : start_date \u0e04\u0e37\u0e2d\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e43\u0e2b\u0e49 DAG \u0e40\u0e23\u0e34\u0e48\u0e21\u0e17\u0e33\u0e07\u0e32\u0e19 \u0e43\u0e0a\u0e49 datetime \u0e2b\u0e23\u0e37\u0e2d airflow.utils.dates \u0e01\u0e47\u0e44\u0e14\u0e49 Line 11 : schedule_interval \u0e04\u0e37\u0e2d\u0e23\u0e2d\u0e1a\u0e01\u0e32\u0e23\u0e23\u0e31\u0e19 DAG \u0e40\u0e0a\u0e48\u0e19 @daily \u0e04\u0e37\u0e2d \u0e43\u0e2b\u0e49\u0e23\u0e31\u0e19\u0e23\u0e32\u0e22\u0e27\u0e31\u0e19, @monthly \u0e04\u0e37\u0e2d\u0e23\u0e31\u0e19\u0e23\u0e32\u0e22\u0e40\u0e14\u0e37\u0e2d\u0e19 Objective \u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 DAG \u0e17\u0e35\u0e48\u0e23\u0e31\u0e19\u0e44\u0e14\u0e49 \u0e41\u0e15\u0e48\u0e44\u0e21\u0e48\u0e21\u0e35\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e48\u0e19\u0e43\u0e14\u0e46 Deploy DAGs \u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c\u0e02\u0e2d\u0e07 MinIO \u0e17\u0e35\u0e48 http://172.17.113.251:9000 \u0e41\u0e25\u0e30\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e17\u0e35\u0e48 Bucket dags \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e25\u0e32\u0e01\u0e44\u0e1f\u0e25\u0e4c dpd_training_<name>.py \u0e1a\u0e19\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e44\u0e1b\u0e43\u0e2a\u0e48\u0e43\u0e19 dags Manage DAGs \u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c Airflow UI \u0e17\u0e35\u0e48 http://airflow.mea.or.th \u0e23\u0e2d\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 2 - 3 \u0e19\u0e32\u0e17\u0e35 \u0e41\u0e25\u0e49\u0e27\u0e25\u0e2d\u0e07\u0e04\u0e49\u0e19\u0e2b\u0e32 DAG \u0e14\u0e49\u0e27\u0e22\u0e0a\u0e37\u0e48\u0e2d dpd_training \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19 DAG \u0e17\u0e35\u0e48\u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 \u0e40\u0e1b\u0e34\u0e14\u0e1b\u0e34\u0e14 DAG \u0e44\u0e14\u0e49\u0e40\u0e25\u0e22 Results \u0e01\u0e14 Switch \u0e02\u0e49\u0e32\u0e07\u0e0b\u0e49\u0e32\u0e22\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e1b\u0e34\u0e14\u0e01\u0e32\u0e23\u0e17\u0e33\u0e07\u0e32\u0e19\u0e02\u0e2d\u0e07 DAG \u0e41\u0e25\u0e30 \u0e14\u0e39\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c Example 2 - Read from MinIO \u0e16\u0e49\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e19\u0e33\u0e44\u0e1f\u0e25\u0e4c\u0e1e\u0e27\u0e01 CSV, JSON \u0e2b\u0e23\u0e37\u0e2d \u0e2d\u0e37\u0e48\u0e19\u0e46 \u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e1c\u0e25\u0e43\u0e19 Airflow \u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19\u0e04\u0e37\u0e2d \u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e44\u0e1f\u0e25\u0e4c\u0e43\u0e19 Bucket \u0e0a\u0e37\u0e48\u0e2d airflow (\u0e40\u0e09\u0e1e\u0e32\u0e30 Data Engineer) \u0e2b\u0e23\u0e37\u0e2d public (\u0e40\u0e2b\u0e47\u0e19\u0e44\u0e14\u0e49\u0e17\u0e38\u0e01\u0e04\u0e19) \u0e40\u0e02\u0e35\u0e22\u0e19 DAG \u0e43\u0e2b\u0e49\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e44\u0e1f\u0e25\u0e4c\u0e08\u0e32\u0e01 MinIO Deploy DAG \u0e1a\u0e19 MinIO Bucket dags \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 DAG \u0e1a\u0e19 Airflow UI Objective \u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 DAG \u0e17\u0e35\u0e48\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01 MinIO \u0e41\u0e25\u0e30\u0e2a\u0e48\u0e07\u0e44\u0e1f\u0e25\u0e4c\u0e17\u0e35\u0e48\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e40\u0e02\u0e49\u0e32\u0e2d\u0e35\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e1e\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19 \u0e01\u0e1f\u0e19. Upload files \u0e40\u0e2d\u0e32\u0e44\u0e1f\u0e25\u0e4c\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e42\u0e22\u0e19\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e43\u0e19 airflow Bucket Create DAG Copy code \u0e14\u0e49\u0e32\u0e19\u0e25\u0e48\u0e32\u0e07\u0e43\u0e2a\u0e48\u0e44\u0e1f\u0e25\u0e4c\u0e0a\u0e37\u0e48\u0e2d dpd_training_minio_<name>.py \u0e16\u0e49\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e14\u0e39\u0e17\u0e35\u0e48 \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e42\u0e14\u0e22\u0e25\u0e30\u0e40\u0e2d\u0e35\u0e22\u0e14\u0e2d\u0e48\u0e32\u0e19\u0e17\u0e35\u0e48 \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e01\u0e32\u0e23\u0e2d\u0e48\u0e32\u0e19\u0e44\u0e1f\u0e25\u0e4c CSV \u0e08\u0e32\u0e01 MinIO MinIO Python SDK dpd_training_minio_chatree.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import airflow import urllib3 from minio import Minio from airflow import DAG from airflow.models import Variable from airflow.operators.email import EmailOperator from airflow.operators.python import PythonOperator # Initialize the DAG with DAG ( dag_id = \"dpd_training_minio_chatree\" , description = \"Download file from MinIO to Airflow Cluster\" , start_date = airflow . utils . dates . days_ago ( 1 ), schedule_interval = \"@daily\" ) as dag : # define python function to be used with PythonOperator def _read_file_from_minio ( obj , bucket , out ): client = Minio ( Variable . get ( 'MINIO_HOST' ), access_key = Variable . get ( 'MINIO_SECRET_ACCESS_KEY' ), secret_key = Variable . get ( 'MINIO_SECRET_KEY' ), secure = False , http_client = urllib3 . ProxyManager ( 'http://meaproxy.mea.or.th:80' ) ) client . fget_object ( bucket_name = bucket , object_name = obj , file_path = out ) download_file = PythonOperator ( task_id = \"download_file\" , python_callable = _read_file_from_minio , op_kwargs = { \"obj\" : 'DPD/crash_catalonia.csv' , \"bucket\" : 'airflow' , \"out\" : '/shared/crash_catalonia.csv' } ) send_file = EmailOperator ( task_id = \"send_file\" , to = [ \"chatree.sr@mea.or.th\" ], subject = \"[{{ ds }}] File downloaded from MinIO\" , html_content = \"\"\" <p>Please find attached</p> \"\"\" , files = [ \"/shared/crash_catalonia.csv\" ] ) download_file >> send_file \u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d Line 5 : \u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e34\u0e14\u0e40\u0e1c\u0e22\u0e04\u0e48\u0e32 \u0e40\u0e0a\u0e48\u0e19 Password, Key \u0e1c\u0e48\u0e32\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e15\u0e48\u0e32\u0e07 Airflow UI > Admin > Variables Line 6-7 : Operator \u0e04\u0e37\u0e2d Code \u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e19\u0e40\u0e02\u0e35\u0e22\u0e19\u0e21\u0e32\u0e43\u0e2b\u0e49 \u0e43\u0e0a\u0e49\u0e41\u0e01\u0e49\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e08\u0e38\u0e14 \u0e40\u0e0a\u0e48\u0e19 EmailOperator \u0e43\u0e0a\u0e49\u0e2a\u0e48\u0e07 Email, PythonOperator \u0e43\u0e0a\u0e49\u0e23\u0e31\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 Python Line 19-25 : \u0e40\u0e02\u0e35\u0e22\u0e19 Function \u0e43\u0e0a\u0e49\u0e2d\u0e49\u0e32\u0e07\u0e16\u0e36\u0e07\u0e15\u0e2d\u0e19\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 PythonOperator \u0e15\u0e31\u0e27 MinIO \u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49 http_client=urllib3.ProxyManager \u0e14\u0e49\u0e27\u0e22\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01 Network \u0e01\u0e1f\u0e19. \u0e15\u0e49\u0e2d\u0e07\u0e1c\u0e48\u0e32\u0e19 Proxy Line 38 : Pass arguments \u0e44\u0e1b\u0e17\u0e35\u0e48 python_callable \u0e14\u0e49\u0e27\u0e22 op_kwargs={} Line 41 : Airflow \u0e21\u0e35 Workers 15 \u0e15\u0e31\u0e27 \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\u0e21\u0e35 File System \u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \u0e2a\u0e32\u0e40\u0e2b\u0e15\u0e38\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49 Path /shared \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 Shared Path \u0e40\u0e14\u0e35\u0e22\u0e27\u0e17\u0e35\u0e48 Worker \u0e17\u0e38\u0e01\u0e15\u0e31\u0e27\u0e40\u0e02\u0e49\u0e32\u0e16\u0e36\u0e07\u0e44\u0e14\u0e49 Line 45 : \u0e2a\u0e48\u0e07\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e41\u0e19\u0e1a\u0e44\u0e1f\u0e25\u0e4c\u0e44\u0e14\u0e49\u0e40\u0e25\u0e22 \u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07 Config SMTP (\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e15\u0e31\u0e49\u0e07\u0e04\u0e48\u0e32\u0e2b\u0e25\u0e31\u0e07\u0e1a\u0e49\u0e32\u0e19\u0e44\u0e27\u0e49\u0e2b\u0e21\u0e14\u0e41\u0e25\u0e49\u0e27) Line 47 : \u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e40\u0e1b\u0e47\u0e19 Email \u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c Line 48 : {{ ds }} \u0e04\u0e37\u0e2d Jinja Template \u0e17\u0e35\u0e48 Airflow \u0e43\u0e0a\u0e49\u0e41\u0e17\u0e19\u0e04\u0e48\u0e32\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e40\u0e0a\u0e48\u0e19 ds \u0e04\u0e37\u0e2d Dag Run Date \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e2d\u0e37\u0e48\u0e19\u0e46\u0e14\u0e39\u0e17\u0e35\u0e48 Template Reference Deploy DAG \u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c\u0e02\u0e2d\u0e07 MinIO \u0e17\u0e35\u0e48 http://172.17.113.251:9000 \u0e41\u0e25\u0e30\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e17\u0e35\u0e48 Bucket dags \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e25\u0e32\u0e01\u0e44\u0e1f\u0e25\u0e4c dpd_training_minio_<name>.py \u0e1a\u0e19\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e44\u0e1b\u0e43\u0e2a\u0e48\u0e43\u0e19 dags Manage DAG \u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c Airflow UI \u0e17\u0e35\u0e48 http://airflow.mea.or.th \u0e23\u0e2d\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 2 - 3 \u0e19\u0e32\u0e17\u0e35 \u0e41\u0e25\u0e49\u0e27\u0e25\u0e2d\u0e07\u0e04\u0e49\u0e19\u0e2b\u0e32 DAG \u0e14\u0e49\u0e27\u0e22\u0e0a\u0e37\u0e48\u0e2d dpd_training \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19 DAG \u0e17\u0e35\u0e48\u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 \u0e40\u0e1b\u0e34\u0e14\u0e1b\u0e34\u0e14 DAG \u0e44\u0e14\u0e49\u0e40\u0e25\u0e22 Results \u0e40\u0e0a\u0e47\u0e04\u0e2b\u0e19\u0e49\u0e32\u0e15\u0e48\u0e32\u0e07 UI \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19\u0e27\u0e48\u0e32 Task \u0e23\u0e31\u0e19\u0e40\u0e2a\u0e23\u0e47\u0e08\u0e2b\u0e21\u0e14\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e0a\u0e47\u0e04\u0e2d\u0e35\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \u0e14\u0e39\u0e44\u0e1f\u0e25\u0e4c\u0e17\u0e35\u0e48\u0e2a\u0e48\u0e07\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e08\u0e32\u0e01 Airflow Example 3 - Pandas & REST API \u0e15\u0e31\u0e07\u0e41\u0e15\u0e48\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e19\u0e35\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e19\u0e44\u0e1b\u0e08\u0e30\u0e44\u0e21\u0e48\u0e41\u0e2a\u0e14\u0e07\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23 Deploy \u0e41\u0e25\u0e30 Manage DAG Objective \u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 DAG \u0e17\u0e35\u0e48 \u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01 EGAT API \u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e1c\u0e25\u0e14\u0e49\u0e27\u0e22 Pandas \u0e1a\u0e31\u0e19\u0e17\u0e36\u0e01\u0e44\u0e1f\u0e25\u0e4c\u0e40\u0e1e\u0e37\u0e48\u0e2d Backup \u0e1a\u0e19 MinIO \u0e08\u0e31\u0e14\u0e40\u0e01\u0e47\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19 Hive \u0e2a\u0e48\u0e07\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e44\u0e1f\u0e25\u0e4c\u0e41\u0e19\u0e1a\u0e43\u0e2b\u0e49\u0e2b\u0e19\u0e48\u0e27\u0e22\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07 Create DAG dpd_training_egat_chatree.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 import airflow import json import urllib3 import requests import numpy as np import pandas as pd from minio import Minio from airflow import DAG from airflow.models import Variable from airflow.operators.email import EmailOperator from airflow.operators.python import PythonOperator from airflow.providers.jdbc.operators.jdbc import JdbcOperator # Custom MEA hooks from mea.hooks.webhdfs import WebHDFSHook # Initialize the DAG with DAG ( dag_id = \"dpd_training_minio_egat_chatree\" , description = \"Store data from EGAT API to Data Platform \" , start_date = airflow . utils . dates . days_ago ( 1 ), template_searchpath = [ \"/shared\" ], schedule_interval = \"@daily\" ) as dag : ############################################################################### # Utility functions; Should be in a separate file. # ############################################################################### def _get_token (): headers = { 'Content-Type' : 'application/json' } data = { \"username\" : Variable . get ( 'DPD_EGAT_API_ACCESS_KEY' ), \"password\" : Variable . get ( 'DPD_EGAT_API_SECRET_KEY' ) } try : res = requests . post ( Variable . get ( 'DPD_EGAT_API_AUTH_URL' ), headers = headers , json = data ) res . raise_for_status () except requests . exceptions . HTTPError as e : raise SystemExit ( e ) return res . json () . get ( 'access_token' ) def _get_data ( api_url , feeder_name , year , month , outfile ): token = _get_token () headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer { token } \" } data = { \"linefeederName\" : feeder_name , \"year\" : year , \"month\" : month } try : res = requests . post ( Variable . get ( api_url ), headers = headers , json = data ) except requests . exceptions . HTTPError as e : raise SystemExit ( e ) with open ( outfile , 'w' ) as f : json . dump ( res . json (), f ) def _get_billing ( feeder_name , year , month , outfile ): _get_data ( 'DPD_EGAT_API_BILLING_URL' , feeder_name , year , month , outfile ) def _get_lp ( feeder_name , year , month , outfile ): _get_data ( 'DPD_EGAT_API_LP_URL' , feeder_name , year , month , outfile ) def _convert_json_to_csv ( infile , outfile ): with open ( infile , 'r' ) as f : records = json . load ( f ) # Convert JSON to DataFrame and save to CSV pd . DataFrame . from_records ( records ) . to_csv ( outfile , index = False ) def _generate_hive_sql ( infile , outfile , db_name , table_name , hdfs_path ): df = pd . read_csv ( infile ) with open ( outfile , 'w' ) as f : f . write ( f 'CREATE EXTERNAL TABLE IF NOT EXISTS { db_name } . { table_name } ( \\n ' ) for col in df . columns : if df [ col ] . dtype == object : f . write ( f ' \\t { col } STRING' ) if df [ col ] . dtype == np . int64 : f . write ( f ' \\t { col } DOUBLE' ) if df [ col ] . dtype == np . float64 : f . write ( f ' \\t { col } DOUBLE' ) if col != df . columns [ - 1 ]: f . write ( ', \\n ' ) f . write ( ') \\n ' ) f . write ( 'ROW FORMAT DELIMITED \\n ' ) f . write ( \"FIELDS TERMINATED BY ',' \\n \" ) f . write ( 'STORED AS TEXTFILE \\n ' ) f . write ( f \"LOCATION ' { hdfs_path } ' \\n \" ) f . write ( \"TBLPROPERTIES ('skip.header.line.count'='1');\" ) def _upload_file_to_hdfs ( infile , hdfs_path ): hdfs = WebHDFSHook ( webhdfs_conn_id = 'sys-hdfs' ) hdfs . load_file ( source = infile , destination = hdfs_path , overwrite = True ) def _upload_file_to_minio ( obj , bucket , file_path ): client = Minio ( Variable . get ( 'MINIO_HOST' ), access_key = Variable . get ( 'MINIO_SECRET_ACCESS_KEY' ), secret_key = Variable . get ( 'MINIO_SECRET_KEY' ), secure = False , http_client = urllib3 . ProxyManager ( 'http://meaproxy.mea.or.th:80' ) ) client . fput_object ( bucket_name = bucket , object_name = obj , file_path = file_path ) ############################################################################### # Tasks # ############################################################################### get_egat_lp = PythonOperator ( task_id = \"get_egat_lp\" , python_callable = _get_lp , op_kwargs = { \"feeder_name\" : \"BK/69 MEA#1 M\" , \"year\" : 2021 , \"month\" : 12 , \"outfile\" : \"/shared/bk69_mea1_m.json\" } ) convert_json_to_csv = PythonOperator ( task_id = \"convert_json_to_csv\" , python_callable = _convert_json_to_csv , op_kwargs = { \"infile\" : \"/shared/bk69_mea1_m.json\" , \"outfile\" : \"/shared/bk69_mea1_m.csv\" } ) upload_file_to_hdfs = PythonOperator ( task_id = \"upload_file_to_hdfs\" , python_callable = _upload_file_to_hdfs , op_kwargs = { \"infile\" : '/shared/bk69_mea1_m.csv' , \"hdfs_path\" : '/user/airflow/dpd-training/egat/bk69_mea1_m.csv' } ) upload_backup_to_minio = PythonOperator ( task_id = \"upload_backup_to_minio\" , python_callable = _upload_file_to_minio , op_kwargs = { 'bucket' : 'airflow' , 'obj' : 'DPD/bk69_mea1_m.csv' , 'file_path' : '/shared/bk69_mea1_m.csv' } ) generate_hive_sql = PythonOperator ( task_id = \"generate_hive_sql\" , python_callable = _generate_hive_sql , op_kwargs = { \"infile\" : '/shared/bk69_mea1_m.csv' , \"outfile\" : '/shared/bk69_mea1_m.sql' , \"db_name\" : 'airflow' , \"table_name\" : 'dpd_training_egat_chatree' , \"hdfs_path\" : '/user/airflow/dpd-training/egat/' } ) create_external_table = JdbcOperator ( task_id = \"create_external_table\" , jdbc_conn_id = 'sys-jdbc-hive' , sql = \"bk69_mea1_m.sql\" ) send_email = EmailOperator ( task_id = \"send_email\" , to = [ \"chatree.sr@mea.or.th\" ], subject = \"[{{ ds }}] EGAT file saved!\" , html_content = \"\"\" <h1 style=\"color: blue;\">Please find attached</h1> <p>Please find attached EGAT data as of {{ ds }}</p> \"\"\" , files = [ \"/shared/bk69_mea1_m.csv\" ] ) get_egat_lp >> convert_json_to_csv >> upload_file_to_hdfs >> generate_hive_sql >> create_external_table >> send_email convert_json_to_csv >> upload_backup_to_minio >> send_email \u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d \u0e2d\u0e30\u0e44\u0e23\u0e17\u0e35\u0e48\u0e17\u0e33\u0e1a\u0e19 Python \u0e44\u0e14\u0e49 \u0e17\u0e33\u0e1a\u0e19 Airflow \u0e44\u0e14\u0e49\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e19 \u0e2a\u0e48\u0e27\u0e19\u0e43\u0e2b\u0e0d\u0e48\u0e08\u0e30\u0e21\u0e35 Operator \u0e40\u0e09\u0e1e\u0e32\u0e30\u0e01\u0e34\u0e08\u0e44\u0e27\u0e49\u0e43\u0e2b\u0e49\u0e41\u0e25\u0e49\u0e27 \u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e23\u0e32\u0e22\u0e01\u0e32\u0e23\u0e44\u0e14\u0e49\u0e17\u0e35\u0e48 Airflow Operators & Hooks \u0e1d\u0e27\u0e17. \u0e1e\u0e31\u0e12\u0e19\u0e32 Custom Hooks & Operators \u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d Data Platform \u0e44\u0e14\u0e49 \u0e40\u0e0a\u0e48\u0e19 mea.hooks.webhdfs Results","title":"Basics"},{"location":"apache-airflow/#basics","text":"","title":"Basics"},{"location":"apache-airflow/#prerequisites","text":"\u0e17\u0e31\u0e01\u0e29\u0e30\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e17\u0e35\u0e48\u0e04\u0e27\u0e23\u0e21\u0e35 Python 3.6+ Basic Apache Airflow Basic databases Linux","title":"Prerequisites"},{"location":"apache-airflow/#airflow-architecture","text":"","title":"Airflow Architecture"},{"location":"apache-airflow/#_1","text":"Airflow \u0e2d\u0e48\u0e32\u0e19 DAGs \u0e08\u0e32\u0e01 Bucket \u0e0a\u0e37\u0e48\u0e2d dags \u0e43\u0e19 MinIO \u0e21\u0e35\u0e01\u0e32\u0e23\u0e0b\u0e34\u0e07\u0e04\u0e4c\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e38\u0e01\u0e46 1 \u0e19\u0e32\u0e17\u0e35 Airflow \u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d\u0e01\u0e31\u0e1a Data Platform Services \u0e2b\u0e25\u0e31\u0e07\u0e1a\u0e49\u0e32\u0e19 \u0e40\u0e0a\u0e48\u0e19 Hive, Impala, HBase, Spark Airflow \u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d\u0e01\u0e31\u0e1a\u0e10\u0e32\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19 MEA \u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19 \u0e40\u0e0a\u0e48\u0e19 SAP SFTP, AMR Oracle Database, OT MSSQL Database, etc. Airflow \u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 Docker \u0e41\u0e25\u0e30\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e41\u0e1a\u0e1a Celery Executor \u0e21\u0e35 15 Workers Airflow Server \u0e21\u0e35 92 vCPUs \u0e41\u0e25\u0e30 312 GB RAM","title":"\u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d"},{"location":"apache-airflow/#_2","text":"\u0e40\u0e02\u0e35\u0e22\u0e19 DAGs \u0e14\u0e49\u0e27\u0e22\u0e20\u0e32\u0e29\u0e32 Python Log in \u0e40\u0e02\u0e49\u0e32\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 MinIO Server \u0e41\u0e25\u0e30\u0e43\u0e2b\u0e49\u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e44\u0e1f\u0e25\u0e4c .py \u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e17\u0e35\u0e48 Bucket \u0e0a\u0e37\u0e48\u0e2d dags . \u0e16\u0e49\u0e32\u0e44\u0e21\u0e48\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14 Airflow \u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e0b\u0e34\u0e07\u0e04\u0e4c DAGs \u0e20\u0e32\u0e22\u0e43\u0e19 2 - 3 \u0e19\u0e32\u0e17\u0e35 \u0e16\u0e49\u0e32\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14\u0e2b\u0e19\u0e49\u0e32\u0e15\u0e48\u0e32\u0e07 UI \u0e08\u0e30\u0e41\u0e08\u0e49\u0e07 \u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e41\u0e25\u0e30\u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 DAGs \u0e44\u0e14\u0e49\u0e17\u0e35\u0e48 Airflow Web UI .","title":"\u0e01\u0e23\u0e30\u0e1a\u0e27\u0e19\u0e07\u0e32\u0e19\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19"},{"location":"apache-airflow/#example-1-simple-dag","text":"","title":"Example 1 - Simple DAG"},{"location":"apache-airflow/#create-dags","text":"Copy code \u0e14\u0e49\u0e32\u0e19\u0e25\u0e48\u0e32\u0e07\u0e41\u0e25\u0e30\u0e40\u0e0b\u0e1f\u0e40\u0e1b\u0e47\u0e19\u0e0a\u0e37\u0e48\u0e2d dpd_training_<name>.py dpd_training_chatree.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import airflow from airflow import DAG from airflow.operators.dummy import DummyOperator # Initialize the DAG with DAG ( dag_id = \"dpd_training_chatree\" , description = \"DAG for learning Apache Airflow\" , start_date = airflow . utils . dates . days_ago ( 1 ), schedule_interval = \"@daily\" ) as dag : task1 = DummyOperator ( task_id = \"task1\" ) task2 = DummyOperator ( task_id = \"task2\" ) task3 = DummyOperator ( task_id = \"task3\" ) task4 = DummyOperator ( task_id = \"task4\" ) task1 >> task2 >> task3 >> task4 \u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d Line 8 : DAG ID \u0e15\u0e49\u0e2d\u0e07 \u0e44\u0e21\u0e48\u0e0b\u0e49\u0e33\u0e01\u0e31\u0e19 (Globally unique) Line 10 : start_date \u0e04\u0e37\u0e2d\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e43\u0e2b\u0e49 DAG \u0e40\u0e23\u0e34\u0e48\u0e21\u0e17\u0e33\u0e07\u0e32\u0e19 \u0e43\u0e0a\u0e49 datetime \u0e2b\u0e23\u0e37\u0e2d airflow.utils.dates \u0e01\u0e47\u0e44\u0e14\u0e49 Line 11 : schedule_interval \u0e04\u0e37\u0e2d\u0e23\u0e2d\u0e1a\u0e01\u0e32\u0e23\u0e23\u0e31\u0e19 DAG \u0e40\u0e0a\u0e48\u0e19 @daily \u0e04\u0e37\u0e2d \u0e43\u0e2b\u0e49\u0e23\u0e31\u0e19\u0e23\u0e32\u0e22\u0e27\u0e31\u0e19, @monthly \u0e04\u0e37\u0e2d\u0e23\u0e31\u0e19\u0e23\u0e32\u0e22\u0e40\u0e14\u0e37\u0e2d\u0e19","title":"Create DAGs"},{"location":"apache-airflow/#objective","text":"\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 DAG \u0e17\u0e35\u0e48\u0e23\u0e31\u0e19\u0e44\u0e14\u0e49 \u0e41\u0e15\u0e48\u0e44\u0e21\u0e48\u0e21\u0e35\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e48\u0e19\u0e43\u0e14\u0e46","title":"Objective"},{"location":"apache-airflow/#deploy-dags","text":"\u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c\u0e02\u0e2d\u0e07 MinIO \u0e17\u0e35\u0e48 http://172.17.113.251:9000 \u0e41\u0e25\u0e30\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e17\u0e35\u0e48 Bucket dags \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e25\u0e32\u0e01\u0e44\u0e1f\u0e25\u0e4c dpd_training_<name>.py \u0e1a\u0e19\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e44\u0e1b\u0e43\u0e2a\u0e48\u0e43\u0e19 dags","title":"Deploy DAGs"},{"location":"apache-airflow/#manage-dags","text":"\u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c Airflow UI \u0e17\u0e35\u0e48 http://airflow.mea.or.th \u0e23\u0e2d\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 2 - 3 \u0e19\u0e32\u0e17\u0e35 \u0e41\u0e25\u0e49\u0e27\u0e25\u0e2d\u0e07\u0e04\u0e49\u0e19\u0e2b\u0e32 DAG \u0e14\u0e49\u0e27\u0e22\u0e0a\u0e37\u0e48\u0e2d dpd_training \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19 DAG \u0e17\u0e35\u0e48\u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 \u0e40\u0e1b\u0e34\u0e14\u0e1b\u0e34\u0e14 DAG \u0e44\u0e14\u0e49\u0e40\u0e25\u0e22","title":"Manage DAGs"},{"location":"apache-airflow/#results","text":"\u0e01\u0e14 Switch \u0e02\u0e49\u0e32\u0e07\u0e0b\u0e49\u0e32\u0e22\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e1b\u0e34\u0e14\u0e01\u0e32\u0e23\u0e17\u0e33\u0e07\u0e32\u0e19\u0e02\u0e2d\u0e07 DAG \u0e41\u0e25\u0e30 \u0e14\u0e39\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c","title":"Results"},{"location":"apache-airflow/#example-2-read-from-minio","text":"\u0e16\u0e49\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e19\u0e33\u0e44\u0e1f\u0e25\u0e4c\u0e1e\u0e27\u0e01 CSV, JSON \u0e2b\u0e23\u0e37\u0e2d \u0e2d\u0e37\u0e48\u0e19\u0e46 \u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e1c\u0e25\u0e43\u0e19 Airflow \u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19\u0e04\u0e37\u0e2d \u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e44\u0e1f\u0e25\u0e4c\u0e43\u0e19 Bucket \u0e0a\u0e37\u0e48\u0e2d airflow (\u0e40\u0e09\u0e1e\u0e32\u0e30 Data Engineer) \u0e2b\u0e23\u0e37\u0e2d public (\u0e40\u0e2b\u0e47\u0e19\u0e44\u0e14\u0e49\u0e17\u0e38\u0e01\u0e04\u0e19) \u0e40\u0e02\u0e35\u0e22\u0e19 DAG \u0e43\u0e2b\u0e49\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e44\u0e1f\u0e25\u0e4c\u0e08\u0e32\u0e01 MinIO Deploy DAG \u0e1a\u0e19 MinIO Bucket dags \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 DAG \u0e1a\u0e19 Airflow UI","title":"Example 2 - Read from MinIO"},{"location":"apache-airflow/#objective_1","text":"\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 DAG \u0e17\u0e35\u0e48\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01 MinIO \u0e41\u0e25\u0e30\u0e2a\u0e48\u0e07\u0e44\u0e1f\u0e25\u0e4c\u0e17\u0e35\u0e48\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e40\u0e02\u0e49\u0e32\u0e2d\u0e35\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e1e\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19 \u0e01\u0e1f\u0e19.","title":"Objective"},{"location":"apache-airflow/#upload-files","text":"\u0e40\u0e2d\u0e32\u0e44\u0e1f\u0e25\u0e4c\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e42\u0e22\u0e19\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e43\u0e19 airflow Bucket","title":"Upload files"},{"location":"apache-airflow/#create-dag","text":"Copy code \u0e14\u0e49\u0e32\u0e19\u0e25\u0e48\u0e32\u0e07\u0e43\u0e2a\u0e48\u0e44\u0e1f\u0e25\u0e4c\u0e0a\u0e37\u0e48\u0e2d dpd_training_minio_<name>.py \u0e16\u0e49\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e14\u0e39\u0e17\u0e35\u0e48 \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e42\u0e14\u0e22\u0e25\u0e30\u0e40\u0e2d\u0e35\u0e22\u0e14\u0e2d\u0e48\u0e32\u0e19\u0e17\u0e35\u0e48 \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e01\u0e32\u0e23\u0e2d\u0e48\u0e32\u0e19\u0e44\u0e1f\u0e25\u0e4c CSV \u0e08\u0e32\u0e01 MinIO MinIO Python SDK dpd_training_minio_chatree.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import airflow import urllib3 from minio import Minio from airflow import DAG from airflow.models import Variable from airflow.operators.email import EmailOperator from airflow.operators.python import PythonOperator # Initialize the DAG with DAG ( dag_id = \"dpd_training_minio_chatree\" , description = \"Download file from MinIO to Airflow Cluster\" , start_date = airflow . utils . dates . days_ago ( 1 ), schedule_interval = \"@daily\" ) as dag : # define python function to be used with PythonOperator def _read_file_from_minio ( obj , bucket , out ): client = Minio ( Variable . get ( 'MINIO_HOST' ), access_key = Variable . get ( 'MINIO_SECRET_ACCESS_KEY' ), secret_key = Variable . get ( 'MINIO_SECRET_KEY' ), secure = False , http_client = urllib3 . ProxyManager ( 'http://meaproxy.mea.or.th:80' ) ) client . fget_object ( bucket_name = bucket , object_name = obj , file_path = out ) download_file = PythonOperator ( task_id = \"download_file\" , python_callable = _read_file_from_minio , op_kwargs = { \"obj\" : 'DPD/crash_catalonia.csv' , \"bucket\" : 'airflow' , \"out\" : '/shared/crash_catalonia.csv' } ) send_file = EmailOperator ( task_id = \"send_file\" , to = [ \"chatree.sr@mea.or.th\" ], subject = \"[{{ ds }}] File downloaded from MinIO\" , html_content = \"\"\" <p>Please find attached</p> \"\"\" , files = [ \"/shared/crash_catalonia.csv\" ] ) download_file >> send_file \u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d Line 5 : \u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e34\u0e14\u0e40\u0e1c\u0e22\u0e04\u0e48\u0e32 \u0e40\u0e0a\u0e48\u0e19 Password, Key \u0e1c\u0e48\u0e32\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e15\u0e48\u0e32\u0e07 Airflow UI > Admin > Variables Line 6-7 : Operator \u0e04\u0e37\u0e2d Code \u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e19\u0e40\u0e02\u0e35\u0e22\u0e19\u0e21\u0e32\u0e43\u0e2b\u0e49 \u0e43\u0e0a\u0e49\u0e41\u0e01\u0e49\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e08\u0e38\u0e14 \u0e40\u0e0a\u0e48\u0e19 EmailOperator \u0e43\u0e0a\u0e49\u0e2a\u0e48\u0e07 Email, PythonOperator \u0e43\u0e0a\u0e49\u0e23\u0e31\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 Python Line 19-25 : \u0e40\u0e02\u0e35\u0e22\u0e19 Function \u0e43\u0e0a\u0e49\u0e2d\u0e49\u0e32\u0e07\u0e16\u0e36\u0e07\u0e15\u0e2d\u0e19\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 PythonOperator \u0e15\u0e31\u0e27 MinIO \u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49 http_client=urllib3.ProxyManager \u0e14\u0e49\u0e27\u0e22\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01 Network \u0e01\u0e1f\u0e19. \u0e15\u0e49\u0e2d\u0e07\u0e1c\u0e48\u0e32\u0e19 Proxy Line 38 : Pass arguments \u0e44\u0e1b\u0e17\u0e35\u0e48 python_callable \u0e14\u0e49\u0e27\u0e22 op_kwargs={} Line 41 : Airflow \u0e21\u0e35 Workers 15 \u0e15\u0e31\u0e27 \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\u0e21\u0e35 File System \u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \u0e2a\u0e32\u0e40\u0e2b\u0e15\u0e38\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49 Path /shared \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 Shared Path \u0e40\u0e14\u0e35\u0e22\u0e27\u0e17\u0e35\u0e48 Worker \u0e17\u0e38\u0e01\u0e15\u0e31\u0e27\u0e40\u0e02\u0e49\u0e32\u0e16\u0e36\u0e07\u0e44\u0e14\u0e49 Line 45 : \u0e2a\u0e48\u0e07\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e41\u0e19\u0e1a\u0e44\u0e1f\u0e25\u0e4c\u0e44\u0e14\u0e49\u0e40\u0e25\u0e22 \u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07 Config SMTP (\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e15\u0e31\u0e49\u0e07\u0e04\u0e48\u0e32\u0e2b\u0e25\u0e31\u0e07\u0e1a\u0e49\u0e32\u0e19\u0e44\u0e27\u0e49\u0e2b\u0e21\u0e14\u0e41\u0e25\u0e49\u0e27) Line 47 : \u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e40\u0e1b\u0e47\u0e19 Email \u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c Line 48 : {{ ds }} \u0e04\u0e37\u0e2d Jinja Template \u0e17\u0e35\u0e48 Airflow \u0e43\u0e0a\u0e49\u0e41\u0e17\u0e19\u0e04\u0e48\u0e32\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e40\u0e0a\u0e48\u0e19 ds \u0e04\u0e37\u0e2d Dag Run Date \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e2d\u0e37\u0e48\u0e19\u0e46\u0e14\u0e39\u0e17\u0e35\u0e48 Template Reference","title":"Create DAG"},{"location":"apache-airflow/#deploy-dag","text":"\u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c\u0e02\u0e2d\u0e07 MinIO \u0e17\u0e35\u0e48 http://172.17.113.251:9000 \u0e41\u0e25\u0e30\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e17\u0e35\u0e48 Bucket dags \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e25\u0e32\u0e01\u0e44\u0e1f\u0e25\u0e4c dpd_training_minio_<name>.py \u0e1a\u0e19\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e44\u0e1b\u0e43\u0e2a\u0e48\u0e43\u0e19 dags","title":"Deploy DAG"},{"location":"apache-airflow/#manage-dag","text":"\u0e40\u0e1b\u0e34\u0e14\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c Airflow UI \u0e17\u0e35\u0e48 http://airflow.mea.or.th \u0e23\u0e2d\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 2 - 3 \u0e19\u0e32\u0e17\u0e35 \u0e41\u0e25\u0e49\u0e27\u0e25\u0e2d\u0e07\u0e04\u0e49\u0e19\u0e2b\u0e32 DAG \u0e14\u0e49\u0e27\u0e22\u0e0a\u0e37\u0e48\u0e2d dpd_training \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19 DAG \u0e17\u0e35\u0e48\u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 \u0e40\u0e1b\u0e34\u0e14\u0e1b\u0e34\u0e14 DAG \u0e44\u0e14\u0e49\u0e40\u0e25\u0e22","title":"Manage DAG"},{"location":"apache-airflow/#results_1","text":"\u0e40\u0e0a\u0e47\u0e04\u0e2b\u0e19\u0e49\u0e32\u0e15\u0e48\u0e32\u0e07 UI \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19\u0e27\u0e48\u0e32 Task \u0e23\u0e31\u0e19\u0e40\u0e2a\u0e23\u0e47\u0e08\u0e2b\u0e21\u0e14\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e0a\u0e47\u0e04\u0e2d\u0e35\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \u0e14\u0e39\u0e44\u0e1f\u0e25\u0e4c\u0e17\u0e35\u0e48\u0e2a\u0e48\u0e07\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e08\u0e32\u0e01 Airflow","title":"Results"},{"location":"apache-airflow/#example-3-pandas-rest-api","text":"\u0e15\u0e31\u0e07\u0e41\u0e15\u0e48\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e19\u0e35\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e19\u0e44\u0e1b\u0e08\u0e30\u0e44\u0e21\u0e48\u0e41\u0e2a\u0e14\u0e07\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23 Deploy \u0e41\u0e25\u0e30 Manage DAG","title":"Example 3 - Pandas &amp; REST API"},{"location":"apache-airflow/#objective_2","text":"\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 DAG \u0e17\u0e35\u0e48 \u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01 EGAT API \u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e1c\u0e25\u0e14\u0e49\u0e27\u0e22 Pandas \u0e1a\u0e31\u0e19\u0e17\u0e36\u0e01\u0e44\u0e1f\u0e25\u0e4c\u0e40\u0e1e\u0e37\u0e48\u0e2d Backup \u0e1a\u0e19 MinIO \u0e08\u0e31\u0e14\u0e40\u0e01\u0e47\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19 Hive \u0e2a\u0e48\u0e07\u0e40\u0e21\u0e25\u0e25\u0e4c\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e44\u0e1f\u0e25\u0e4c\u0e41\u0e19\u0e1a\u0e43\u0e2b\u0e49\u0e2b\u0e19\u0e48\u0e27\u0e22\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07","title":"Objective"},{"location":"apache-airflow/#create-dag_1","text":"dpd_training_egat_chatree.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 import airflow import json import urllib3 import requests import numpy as np import pandas as pd from minio import Minio from airflow import DAG from airflow.models import Variable from airflow.operators.email import EmailOperator from airflow.operators.python import PythonOperator from airflow.providers.jdbc.operators.jdbc import JdbcOperator # Custom MEA hooks from mea.hooks.webhdfs import WebHDFSHook # Initialize the DAG with DAG ( dag_id = \"dpd_training_minio_egat_chatree\" , description = \"Store data from EGAT API to Data Platform \" , start_date = airflow . utils . dates . days_ago ( 1 ), template_searchpath = [ \"/shared\" ], schedule_interval = \"@daily\" ) as dag : ############################################################################### # Utility functions; Should be in a separate file. # ############################################################################### def _get_token (): headers = { 'Content-Type' : 'application/json' } data = { \"username\" : Variable . get ( 'DPD_EGAT_API_ACCESS_KEY' ), \"password\" : Variable . get ( 'DPD_EGAT_API_SECRET_KEY' ) } try : res = requests . post ( Variable . get ( 'DPD_EGAT_API_AUTH_URL' ), headers = headers , json = data ) res . raise_for_status () except requests . exceptions . HTTPError as e : raise SystemExit ( e ) return res . json () . get ( 'access_token' ) def _get_data ( api_url , feeder_name , year , month , outfile ): token = _get_token () headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer { token } \" } data = { \"linefeederName\" : feeder_name , \"year\" : year , \"month\" : month } try : res = requests . post ( Variable . get ( api_url ), headers = headers , json = data ) except requests . exceptions . HTTPError as e : raise SystemExit ( e ) with open ( outfile , 'w' ) as f : json . dump ( res . json (), f ) def _get_billing ( feeder_name , year , month , outfile ): _get_data ( 'DPD_EGAT_API_BILLING_URL' , feeder_name , year , month , outfile ) def _get_lp ( feeder_name , year , month , outfile ): _get_data ( 'DPD_EGAT_API_LP_URL' , feeder_name , year , month , outfile ) def _convert_json_to_csv ( infile , outfile ): with open ( infile , 'r' ) as f : records = json . load ( f ) # Convert JSON to DataFrame and save to CSV pd . DataFrame . from_records ( records ) . to_csv ( outfile , index = False ) def _generate_hive_sql ( infile , outfile , db_name , table_name , hdfs_path ): df = pd . read_csv ( infile ) with open ( outfile , 'w' ) as f : f . write ( f 'CREATE EXTERNAL TABLE IF NOT EXISTS { db_name } . { table_name } ( \\n ' ) for col in df . columns : if df [ col ] . dtype == object : f . write ( f ' \\t { col } STRING' ) if df [ col ] . dtype == np . int64 : f . write ( f ' \\t { col } DOUBLE' ) if df [ col ] . dtype == np . float64 : f . write ( f ' \\t { col } DOUBLE' ) if col != df . columns [ - 1 ]: f . write ( ', \\n ' ) f . write ( ') \\n ' ) f . write ( 'ROW FORMAT DELIMITED \\n ' ) f . write ( \"FIELDS TERMINATED BY ',' \\n \" ) f . write ( 'STORED AS TEXTFILE \\n ' ) f . write ( f \"LOCATION ' { hdfs_path } ' \\n \" ) f . write ( \"TBLPROPERTIES ('skip.header.line.count'='1');\" ) def _upload_file_to_hdfs ( infile , hdfs_path ): hdfs = WebHDFSHook ( webhdfs_conn_id = 'sys-hdfs' ) hdfs . load_file ( source = infile , destination = hdfs_path , overwrite = True ) def _upload_file_to_minio ( obj , bucket , file_path ): client = Minio ( Variable . get ( 'MINIO_HOST' ), access_key = Variable . get ( 'MINIO_SECRET_ACCESS_KEY' ), secret_key = Variable . get ( 'MINIO_SECRET_KEY' ), secure = False , http_client = urllib3 . ProxyManager ( 'http://meaproxy.mea.or.th:80' ) ) client . fput_object ( bucket_name = bucket , object_name = obj , file_path = file_path ) ############################################################################### # Tasks # ############################################################################### get_egat_lp = PythonOperator ( task_id = \"get_egat_lp\" , python_callable = _get_lp , op_kwargs = { \"feeder_name\" : \"BK/69 MEA#1 M\" , \"year\" : 2021 , \"month\" : 12 , \"outfile\" : \"/shared/bk69_mea1_m.json\" } ) convert_json_to_csv = PythonOperator ( task_id = \"convert_json_to_csv\" , python_callable = _convert_json_to_csv , op_kwargs = { \"infile\" : \"/shared/bk69_mea1_m.json\" , \"outfile\" : \"/shared/bk69_mea1_m.csv\" } ) upload_file_to_hdfs = PythonOperator ( task_id = \"upload_file_to_hdfs\" , python_callable = _upload_file_to_hdfs , op_kwargs = { \"infile\" : '/shared/bk69_mea1_m.csv' , \"hdfs_path\" : '/user/airflow/dpd-training/egat/bk69_mea1_m.csv' } ) upload_backup_to_minio = PythonOperator ( task_id = \"upload_backup_to_minio\" , python_callable = _upload_file_to_minio , op_kwargs = { 'bucket' : 'airflow' , 'obj' : 'DPD/bk69_mea1_m.csv' , 'file_path' : '/shared/bk69_mea1_m.csv' } ) generate_hive_sql = PythonOperator ( task_id = \"generate_hive_sql\" , python_callable = _generate_hive_sql , op_kwargs = { \"infile\" : '/shared/bk69_mea1_m.csv' , \"outfile\" : '/shared/bk69_mea1_m.sql' , \"db_name\" : 'airflow' , \"table_name\" : 'dpd_training_egat_chatree' , \"hdfs_path\" : '/user/airflow/dpd-training/egat/' } ) create_external_table = JdbcOperator ( task_id = \"create_external_table\" , jdbc_conn_id = 'sys-jdbc-hive' , sql = \"bk69_mea1_m.sql\" ) send_email = EmailOperator ( task_id = \"send_email\" , to = [ \"chatree.sr@mea.or.th\" ], subject = \"[{{ ds }}] EGAT file saved!\" , html_content = \"\"\" <h1 style=\"color: blue;\">Please find attached</h1> <p>Please find attached EGAT data as of {{ ds }}</p> \"\"\" , files = [ \"/shared/bk69_mea1_m.csv\" ] ) get_egat_lp >> convert_json_to_csv >> upload_file_to_hdfs >> generate_hive_sql >> create_external_table >> send_email convert_json_to_csv >> upload_backup_to_minio >> send_email \u0e08\u0e38\u0e14\u0e2a\u0e33\u0e04\u0e31\u0e0d \u0e2d\u0e30\u0e44\u0e23\u0e17\u0e35\u0e48\u0e17\u0e33\u0e1a\u0e19 Python \u0e44\u0e14\u0e49 \u0e17\u0e33\u0e1a\u0e19 Airflow \u0e44\u0e14\u0e49\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e19 \u0e2a\u0e48\u0e27\u0e19\u0e43\u0e2b\u0e0d\u0e48\u0e08\u0e30\u0e21\u0e35 Operator \u0e40\u0e09\u0e1e\u0e32\u0e30\u0e01\u0e34\u0e08\u0e44\u0e27\u0e49\u0e43\u0e2b\u0e49\u0e41\u0e25\u0e49\u0e27 \u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e23\u0e32\u0e22\u0e01\u0e32\u0e23\u0e44\u0e14\u0e49\u0e17\u0e35\u0e48 Airflow Operators & Hooks \u0e1d\u0e27\u0e17. \u0e1e\u0e31\u0e12\u0e19\u0e32 Custom Hooks & Operators \u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d Data Platform \u0e44\u0e14\u0e49 \u0e40\u0e0a\u0e48\u0e19 mea.hooks.webhdfs","title":"Create DAG"},{"location":"apache-airflow/#results_2","text":"","title":"Results"},{"location":"apache-airflow/databases/","text":"Databases \u0e01\u0e32\u0e23\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d Databases \u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d \u0e40\u0e02\u0e35\u0e22\u0e19 Python Function \u0e41\u0e25\u0e30\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a PythonOperator \u0e41\u0e19\u0e30\u0e19\u0e33 \u0e43\u0e0a\u0e49 sqlalchemy \u0e43\u0e0a\u0e49\u0e44\u0e14\u0e49\u0e01\u0e31\u0e1a\u0e2b\u0e25\u0e32\u0e22 Database \u0e40\u0e0a\u0e48\u0e19 PostgreSQL, MySQL, Oracle, MSSQL PostgreSQL \u0e43\u0e0a\u0e49 psycopg2 MSSQL \u0e43\u0e0a\u0e49 pymssql MySQL \u0e43\u0e0a\u0e49 mysql-connector-python MongoDB \u0e43\u0e0a\u0e49 pymongo Neo4j \u0e43\u0e0a\u0e49 neo4j Redis \u0e43\u0e0a\u0e49 redis \u0e16\u0e49\u0e32 Database \u0e2a\u0e19\u0e31\u0e1a\u0e2a\u0e19\u0e38\u0e19 ODBC \u0e43\u0e0a\u0e49 pyodbc \u0e10\u0e32\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 TIBCO Data Virtualization \u0e43\u0e0a\u0e49 pyodbc \u0e43\u0e0a\u0e49 Airflow Operators PostgreSQL MongoDB MSSQL MySQL Redis Neo4j \u0e01\u0e32\u0e23\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d Data Platform \u0e23\u0e32\u0e22\u0e25\u0e30\u0e40\u0e2d\u0e35\u0e22\u0e14\u0e2d\u0e48\u0e32\u0e19\u0e17\u0e35\u0e48 Data Platform Apache Airflow \u0e41\u0e15\u0e48\u0e42\u0e14\u0e22\u0e1b\u0e01\u0e15\u0e34 Operators \u0e02\u0e2d\u0e07 Airflow \u0e08\u0e30\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e23\u0e30\u0e1a\u0e1a\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49 \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e15\u0e34\u0e14\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 Policies \u0e41\u0e25\u0e30 Security Kerberos \u0e42\u0e14\u0e22\u0e2a\u0e23\u0e38\u0e1b HDFS \u0e43\u0e0a\u0e49 WebHDFS Hook Hive/Impala \u0e43\u0e0a\u0e49 JDBCOperator Spark \u0e43\u0e0a\u0e49 LivyOperator Sqoop \u0e43\u0e0a\u0e49 SSHOperator HBase \u0e43\u0e0a\u0e49 PythonOperator \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a HBase REST API \u0e2b\u0e23\u0e37\u0e2d\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e1c\u0e48\u0e32\u0e19 Apache Phoenix \u0e14\u0e49\u0e27\u0e22 phoenixdb","title":"Databases"},{"location":"apache-airflow/databases/#databases","text":"","title":"Databases"},{"location":"apache-airflow/databases/#databases_1","text":"\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d \u0e40\u0e02\u0e35\u0e22\u0e19 Python Function \u0e41\u0e25\u0e30\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a PythonOperator \u0e41\u0e19\u0e30\u0e19\u0e33 \u0e43\u0e0a\u0e49 sqlalchemy \u0e43\u0e0a\u0e49\u0e44\u0e14\u0e49\u0e01\u0e31\u0e1a\u0e2b\u0e25\u0e32\u0e22 Database \u0e40\u0e0a\u0e48\u0e19 PostgreSQL, MySQL, Oracle, MSSQL PostgreSQL \u0e43\u0e0a\u0e49 psycopg2 MSSQL \u0e43\u0e0a\u0e49 pymssql MySQL \u0e43\u0e0a\u0e49 mysql-connector-python MongoDB \u0e43\u0e0a\u0e49 pymongo Neo4j \u0e43\u0e0a\u0e49 neo4j Redis \u0e43\u0e0a\u0e49 redis \u0e16\u0e49\u0e32 Database \u0e2a\u0e19\u0e31\u0e1a\u0e2a\u0e19\u0e38\u0e19 ODBC \u0e43\u0e0a\u0e49 pyodbc \u0e10\u0e32\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 TIBCO Data Virtualization \u0e43\u0e0a\u0e49 pyodbc \u0e43\u0e0a\u0e49 Airflow Operators PostgreSQL MongoDB MSSQL MySQL Redis Neo4j","title":"\u0e01\u0e32\u0e23\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d Databases"},{"location":"apache-airflow/databases/#data-platform","text":"\u0e23\u0e32\u0e22\u0e25\u0e30\u0e40\u0e2d\u0e35\u0e22\u0e14\u0e2d\u0e48\u0e32\u0e19\u0e17\u0e35\u0e48 Data Platform Apache Airflow \u0e41\u0e15\u0e48\u0e42\u0e14\u0e22\u0e1b\u0e01\u0e15\u0e34 Operators \u0e02\u0e2d\u0e07 Airflow \u0e08\u0e30\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e23\u0e30\u0e1a\u0e1a\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49 \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e15\u0e34\u0e14\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 Policies \u0e41\u0e25\u0e30 Security Kerberos \u0e42\u0e14\u0e22\u0e2a\u0e23\u0e38\u0e1b HDFS \u0e43\u0e0a\u0e49 WebHDFS Hook Hive/Impala \u0e43\u0e0a\u0e49 JDBCOperator Spark \u0e43\u0e0a\u0e49 LivyOperator Sqoop \u0e43\u0e0a\u0e49 SSHOperator HBase \u0e43\u0e0a\u0e49 PythonOperator \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a HBase REST API \u0e2b\u0e23\u0e37\u0e2d\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e1c\u0e48\u0e32\u0e19 Apache Phoenix \u0e14\u0e49\u0e27\u0e22 phoenixdb","title":"\u0e01\u0e32\u0e23\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d Data Platform"},{"location":"apache-airflow/source/","text":"Source \u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14 Source Code","title":"Source"},{"location":"apache-airflow/source/#source","text":"\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14 Source Code","title":"Source"}]}